\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\usepackage{tikz}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}

\input{header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% title info

\title{Forest of Eden}
\author{\href{https://github.com/aaronpeikert/}{Aaron Peikert}}
\institute{Humboldt{\textendash}Universität zu Berlin}
\date{
\scriptsize {\lolit Slides:} \href{https://github.com/aaronpeikert/ForestEden}{\tt \scriptsize
  \color{foreground} https://github.com/aaronpeikert/ForestEden}
}


\begin{document}

% title slide
{
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

  \vfill \hfill \includegraphics[height=6mm]{Figs/cc-zero.png} \vspace*{-1cm}
  
  \note{These Slides are for a short Talk (10min) intended for a beginner audience where no statistical knowledge can be expected. 

    Source: {\tt https://github.com/aaronpeikert/ForestEden}
}}
}

\begin{frame}[c]{The Great Divide}
  \begin{center}
  \large
  ``There are two kinds of people:\\
  those who divide everything into two kinds\\
  and those who don’t."
  \end{center}
  \hfill {\textendash} (paraphrasing) \lolit \href{http://hdl.handle.net/2027/mdp.39015032024203?urlappend=\%3Bseq=203}{Robert Benchley}
  \note{
    This epigram captures the unreasonable assumption behind trees that everyhing is a binary class. At the same time a lot of things can be captured in two classes. This fundamental assumption is in all, but the most trivial cases, false. However it proves to be most versatile and is what centrally drives decision trees.
  }
\end{frame}

\begin{frame}[c]{Goal}
  \begin{center}
  \large
  Let's stick to this binary world\\
  \note{
    Imagine our goal is to predict to which kind a person belongs. For that purpose we get information about that person.\\
    Translated in ML slang this is a classification task, where the class of an instance is to be predicted by certain features. 
  }
  \end{center}
\end{frame}

\begin{frame}[c]{The Great Divide}
  \large
  \onslide<1-|handout:1>\textcolor<5-|handout:0>{lolit}{If \textcolor<2,3,4|handout:1->{lolit}{condition}, then \textcolor<3,4|handout:1->{nvhilit}{class}, else \textcolor<4|handout:1->{vhilit}{other class}.} \\
  \onslide<5-|handout:1> \textcolor<handout:1->{nvhilit}{Class} and \textcolor<handout:1->{vhilit}{other class} can again be another condition:\\
  \onslide<6-|handout:1>Resulting in \textcolor<handout:1->{hilit}{yet another class}.\\
  \onslide<7|handout:0>\Huge Confused?
  \note{
    These simple if-condition-then-statements are natural to use. At the same time by combining many of such statements, complicated relations can be captured.
  }
\end{frame}

\begin{frame}[fragile, c]{Metaphor}
  \tikzset{
    treenode/.style = {shape=rectangle, rounded corners,
                       draw, align=center,
                       text=background,
                       color=lolit},
    root/.style     = {treenode},
    env/.style      = {treenode},
    level 1/.style={sibling distance=7em},
    level 2/.style={sibling distance=5em},
    level 3/.style={sibling distance=3em},
    level 4/.style={sibling distance=7em},
    class1/.style      = {treenode, color=hilit},
    class2/.style      = {treenode, color=nvhilit},
    dummy/.style    = {circle,draw}
  }
  \begin{tikzpicture}
    [
      grow                    = right,
      sibling distance        = 3em,
      level distance          = 6em,
      edge from parent/.style = {draw, -latex},
      every node/.style       = {font=\footnotesize},
      sloped
    ]
    \node [root] {initial condition}
      child { node [env] {condition 1} 
        child{ node [env]{condition 1.2}
          child{ node [class1]{class 1}}
          child{ node [class2]{class 2}}}
        child{ node [env] {condition 1.1}
          child{ node [class1]{class 1}}
          child{ node [class2]{class 2}}}}
      child { node [class1] {class 1}};
  \end{tikzpicture}
    \note{
    In the form of trees, really complicated statements can be communicated effectively. By traversing down the tree, taking turns at the nodes, the class at the end note is assigned. 
  }
\end{frame}


\begin{frame}[c]{Garden Eden of Algorythms}
  \large
  \begin{center}
  \only<1|handout:0>{Imagine growing these trees automatically ...\\}
  \only<2|handout:1>{``With Decision Trees you can have your cake and eat it too."\\}
  \Large
  \only<3|handout:0>{Transparent}
  \only<4|handout:0>{Easy to understand}
  \only<5|handout:0>{Natural formulation}
  \only<6|handout:0>{Dirt cheap}
  \only<7|handout:0>{High capacity}
  \only<8|handout:0>{High generalizability}
  \only<9|handout:0>{Can model (almost) anything}
  \end{center}
  \note{
  Decision Trees have the following very fortunate properties:
  \begin{itemize}
  \item Transparent, all ``parameters" have a direkt meaning
  \item Easy to understand, the tree can be translated in ``If \ldots then \ldots" \textendash Statements
  \item Natural formulation, these statements are in much use. They are often employed to convey expert knowledge to non-experts.
  \item Dirt cheap in terms of computational costs
  \item High capacity or high generalizability, whatever is more appropriate
  \item Many data generating processes can be fitted, depending on the choosen capacity even all
  \end{itemize}
  }
\end{frame}

\begin{frame}[c]{Path to Garden Eden}
  \onslide<1-2|handout: 1>1. How can these \textcolor{hilit}{conditions} be constructed?\\
  \onslide<2|handout: 1>2. In which  \textcolor{vhilit}{order} should they apply?\\
  \onslide<3|handout: 0>The path out of \textcolor{lolit}{Garden Eden}\\is the path to \textcolor{lolit}{Forest Eden}.\\ 
  \note{
    In order to construct a tree automatically one needs to find an algorythm to determine which conditions are to be used and in which order.
  }
\end{frame}

\begin{frame}[c]{Rolemodel Devil}
  \Large
  \begin{center}
    \only<1|handout: 0>{Do it like the \textcolor{lolit}{Devil}\\}
    \only<2|handout: 0>{1. Seek \textcolor{hilit}{differences} and \textcolor{hilit}{inequality}\\}
    \only<3>{2. Be \textcolor{vhilit}{greedy}\\}
    \only<4>{1. differences = \textcolor{hilit}{conditions}\\
             2. greedy = \textcolor{vhilit}{order}}
  \end{center}
\end{frame}

\begin{frame}[c]{Question I:}
\begin{center}
  \Large \textcolor{lolit}{In a nutshell:} How to build a decision tree?
\end{center}
\end{frame}

\begin{frame}[c]{Moody Devil}
  \begin{center}
    \onslide<1-2>Devil = moody
    \onslide<2> = 
    \onslide<2-3>high entropy 
    \onslide<3> = 
    \onslide<3>always bad?\\
  \end{center}
  \note{
    Trees often change fundamentally even when the input data is only slightly disturbed. Even though this is often not desirable, this characteristic can be used to grow many different trees. How? See the next slides.
  }
\end{frame}

\begin{frame}[c]{Question II:}
\begin{center}
  \Large \textcolor{lolit}{In a nutshell:}\\ Why do have trees "high variance"?
\end{center}
\end{frame}

\begin{frame}[fragile]{The other side of the coin}
  \begin{center}
    \onslide<1-2>{imagine there are magical coins \\}
    \onslide<2>{you are allowed to toss them 10 times \\}
    \only<3->{
      \onslide<3->Which estimate can you trust the most?\\
      \onslide<4->Coin: $\langle 0.5\rangle$\\
      \onslide<5->Pair: $\langle 0.3, 0.7 \rangle$\\
      \hspace{10mm}
    }
  \end{center}
\end{frame}

\begin{frame}[fragile]
  \begin{center}
  \begin{minipage}{\textwidth}
    \begin{lstlisting}
      k <- 10
      n <- 10000
      coin <- rbinom(n, k, .5)/k
      pair <- cbind(rbinom(n, k, .3), rbinom(n, k, .7))/k
      pair <- rowMeans(pair)
      experiment <- data.frame(coin, pair)
     \end{lstlisting}
   \end{minipage}
   \includegraphics{./Figs/coin}
   \end{center}
\end{frame}

\begin{frame}
  \begin{center}
  \includegraphics{./Figs/coin_cor}
  \end{center}
\end{frame}

\begin{frame}[c]{Key Points}
  \onslide<1-2> 1. Variance can be reduced by sampling more...\\
  \onslide<2> 2. ...when samples are fairly \textcolor{hilit}{uncorrelated}.\\
  \centering
  \onslide<3> \Large How to build \textcolor{hilit}{uncorrelated} trees?
\end{frame}

\begin{frame}[c]{Pertubating Data}
    \onslide<1-3>Trees are volatile due to \textcolor<1>{hilit}{greediness} \& \textcolor<1>{vhilit}{maximizing}\\
    \onslide<2-3>slightly different data → \only<2>{drastically}\only<3>{\textcolor{hilit}{uncorrelated}} \only<2>{different} trees\\
    \onslide<4->1. \textcolor{lolit}{(Re-)}sampling \textcolor{hilit}{cases}\\
    \onslide<5->2. sampling \textcolor{lolit}{(transformed)} \textcolor{vhilit}{variables}\\
\end{frame}

\begin{frame}[c]{Pertubating Trees}
    \onslide<1->1. \textcolor{lolit}{randomize} data at each split\\
    \onslide<2->2. \textcolor{lolit}{randomize} subtree order\\
    \onslide<3->3. \textcolor{lolit}{randomize} splitpoints\\
\end{frame}

\begin{frame}[c]{Question III:}
\begin{center}
  \Large \textcolor{lolit}{In a nutshell:}\\ Why does introduced randomness reduce variance in forests?
\end{center}
\end{frame}

\begin{frame}[c]{Answer: 42}
\begin{center}
  \onslide<1-2>randomness → \textcolor{vhilit}{$+$}variance tree\\
  \onslide<2>randomness → \textcolor{hilit}{$-$}correlation → \textcolor{hilit}{$-$}variance avg\\
  \onslide<3>\textcolor{vhilit}{$+$}variance tree \textcolor{hilit}{$-$} variance avg \textcolor{hilit}{$<0$}\\
\end{center}
\end{frame}

\begin{frame}[c]{The classic Random Forrest}
    Leo Breiman (2001) proposed:\\
    resampling cases for trees \onslide<2->{→ \textcolor<3>{hilit}{b}\textcolor<3>{lolit}{ootstrap} \textcolor<3>{hilit}{agg}\textcolor<3>{lolit}{regat}\textcolor<3>{hilit}{ing }}\newline
    sample variables for splits
\end{frame}

\begin{frame}[c]{Variance}
  \begin{center}
  \large
  ``Das Unbedeutende ist so beduetend\\
  wie irgendetwas anderes."
  \end{center}
  \hfill {\textendash} Walt Whitman
  \textcolor{lolit}{
    \begin{center}
    \large
  ``The unimportant is as important as\\
  anything else"
    \end{center}
  \hfill {\textendash} Walt Whitman}
\end{frame}

\begin{frame}[c]{Encore}
  \Large
  1. extremely randomized trees\\
  2. variable importance\\
\end{frame}

\begin{frame}[c]{extremely randomized trees}
  \begin{enumerate}
    \item at each node
    \begin{enumerate}
       \item choose \textcolor<2>{hilit}{randomly} $k$ variables ($k = \sqrt{n}$)
       \item for each $variable_k$
       \begin{enumerate}
          \item choose \textcolor<2>{hilit}{randomly} cutpoint over $range(variable_k)$
        \end{enumerate}
        \item select best $variable_k$ \textcolor<3>{vhilit}{considering full sample}
    \end{enumerate}
    \item return subtree
 \end{enumerate}
\end{frame}

\begin{frame}[c]{Variable importance}
  for each $variable_i$
  $$\frac{\sum_{m=0}^{M_i} gain_{purity}^i}{M}$$
  where $M_i$ sum of splits where $variable_i$ was considered
\end{frame}

\begin{frame}

\Large

\vspace{10mm}

\scriptsize {\lolit Source:} \href{https://github.com/aaronpeikert/ForestEden}{\tt \scriptsize
  \color{foreground} https://github.com/aaronpeikert/ForestEden} \quad
\includegraphics[height=5mm]{Figs/cc-zero.png}

\vspace{10mm}

\scriptsize {\lolit Slidedesign:} \href{https://github.com/kbroman/Talk_ReproRes}{\tt \scriptsize
  \color{foreground} shamelessly copied from \emph{Karl Broman} \quad}

\vspace{10mm}

\scriptsize {\lolit Github:} \href{https://github.com/aaronpeikert/}{\tt \color{foreground} github.com/aaronpeikert/}

\vspace{10mm}

\scriptsize {\lolit Mail:} \tt aaron.peikert@posteo.de

\note{
  Here's where you can find me, as well as the slides for this talk.
}
\end{frame}

\end{document}
